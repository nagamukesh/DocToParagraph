{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# USING HUGGING FACE TRANSFORMERS"
      ],
      "metadata": {
        "id": "ao-v-zxrdzl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on Hugging Face NLP Course Chapter 2"
      ],
      "metadata": {
        "id": "n7Pl2EDxd4OI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the pretrained facebook/bart-large-cnn model for now"
      ],
      "metadata": {
        "id": "YWrto2CUO4sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "checkpoint = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBH1h292I1Mx",
        "outputId": "19480cd3-1920-48bb-dfd8-cc3a99c92f30"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tokenizer.tokenize(seq) gives a list of strings, each string being a token  \n",
        "- tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")  \n",
        "The output of this call is a dictionary containing the tokenized sequences ('input_ids') and the attention masks (attention_mask).\n",
        "eg.  \n",
        "tokens = {  \n",
        "'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3402,  2005,  1037,  8574,  2607,  2026,  2878,  2166,  1012,   102],  \n",
        "[  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),  \n",
        "'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  \n",
        "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "}"
      ],
      "metadata": {
        "id": "xFVCS_DdVnGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we handle batches with inputs of different sizes, we have to perform 3 things:  \n",
        "Padding  \n",
        "Attention masking  \n",
        "Truncation"
      ],
      "metadata": {
        "id": "l0s3frCJTt_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the texts to summarize\n",
        "sequences = [\n",
        "    \"Artificial intelligence (AI) is a field of computer science that aims to create machines capable of intelligent behavior. It has become an essential part of the technology industry, providing the heavy lifting for many of the most challenging problems in computer science. AI is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and self-correction. Particular applications of AI include expert systems, speech recognition, and machine vision. AI can be categorized into two types: narrow AI, which is designed to perform a narrow task (e.g., facial recognition or internet searches), and general AI, which is designed to perform any intellectual task that a human can do. Researchers are working on developing technologies that can help address global challenges such as climate change, healthcare, and education. AI has the potential to revolutionize many sectors of the economy, including finance, healthcare, and transportation, by enabling more efficient and effective operations. However, AI also raises ethical and societal concerns, such as job displacement, privacy, and the potential for biased decision-making. To address these concerns, it is essential to establish robust frameworks for the development and deployment of AI technologies, ensuring that they are used responsibly and ethically.\",\n",
        "    \"Climate change is one of the most pressing issues of our time. It refers to significant changes in global temperatures and weather patterns over time. While climate change is a natural phenomenon, scientific evidence shows that human activities, particularly the burning of fossil fuels, deforestation, and industrial processes, have accelerated the rate of climate change. These activities increase the concentration of greenhouse gases in the atmosphere, leading to global warming and associated impacts such as rising sea levels, more frequent and severe weather events, and disruptions to ecosystems and biodiversity. Addressing climate change requires a comprehensive approach that includes reducing greenhouse gas emissions, transitioning to renewable energy sources, improving energy efficiency, and implementing sustainable land use and forest management practices. International cooperation is crucial in tackling climate change, as it is a global challenge that transcends national borders. Agreements such as the Paris Agreement aim to unite countries in their efforts to mitigate and adapt to climate change by setting targets for reducing emissions and providing support to developing countries. Public awareness and education are also vital in promoting sustainable practices and encouraging individuals and communities to take action. By working together, we can reduce the impact of climate change and protect the planet for future generations.\",\n",
        "    \"The history of the Internet is a fascinating journey of technological innovation and collaboration that has transformed the way we communicate, access information, and conduct business. The origins of the Internet can be traced back to the 1960s when the United States Department of Defense developed the ARPANET, a network that allowed researchers to share data and resources securely. ARPANET introduced key technologies such as packet switching and the TCP/IP protocol suite, which became the foundation of the modern Internet. During the 1970s and 1980s, academic institutions and government organizations began to develop their own networks, which eventually interconnected, forming a larger network of networks. The introduction of the World Wide Web in the early 1990s by Tim Berners-Lee revolutionized the Internet by providing a user-friendly interface for accessing and sharing information. The Web's rapid growth was fueled by the development of web browsers, search engines, and e-commerce platforms, making the Internet accessible to the general public. As the Internet continued to evolve, it enabled the rise of social media, cloud computing, and the Internet of Things (IoT), connecting billions of devices and creating new opportunities for innovation and economic growth. Today, the Internet is an integral part of our daily lives, impacting everything from communication and education to entertainment and healthcare. Despite its many benefits, the Internet also presents challenges such as cybersecurity threats, privacy concerns, and digital divide issues. Addressing these challenges requires ongoing efforts to enhance security, protect user data, and ensure equitable access to digital technologies.\"\n",
        "]\n",
        "# Tokenize the input text\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYyvRmynI5to",
        "outputId": "52d211f1-bf94-4a3c-ead6-ff38ba7bf297"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The order of the language modeling pipeline:  \n",
        "The tokenizer handles text and returns IDs.  \n",
        "The model handles these IDs and outputs a prediction.   \n",
        "The tokenizer can then be used once again to convert these predictions back to some text.  \n",
        "THE TOKENIZER HANDLES BOTH TOKENIZING (tokenizer.tokenize()) AND DETOKENIZING (tokenizer.decode())"
      ],
      "metadata": {
        "id": "3CjABzZQeHOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the summaries with specified parameters\n",
        "summary_ids = model.generate(\n",
        "    **tokens,\n",
        "    max_length=100,\n",
        "    min_length=50,\n",
        "    num_beams=4,\n",
        "    length_penalty=0.5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# Decode the summary\n",
        "summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "E5dseRoUI7sT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output = model(**tokens)  \n",
        "We get raw outputs containing various attributes like logits  \n",
        "We can apply a softmax on logits to soften it"
      ],
      "metadata": {
        "id": "5YQ_HlfDU9Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the summaries\n",
        "for i, sum in enumerate(summary):\n",
        "    print(f\"Summary {i + 1}: {sum}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dmK_-TvKlov",
        "outputId": "8c633071-e0a3-4785-8f26-558347b40a05"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary 1: Artificial intelligence is the simulation of human intelligence processes by machines. Particular applications of AI include expert systems, speech recognition, and machine vision. AI has the potential to revolutionize many sectors of the economy. It also raises ethical and societal concerns, such as job displacement and privacy.\n",
            "Summary 2: Climate change is one of the most pressing issues of our time. It refers to significant changes in global temperatures and weather patterns. Human activities, particularly the burning of fossil fuels, deforestation, and industrial processes, have accelerated the rate of climate change.\n",
            "Summary 3: The Internet is an integral part of our daily lives, impacting everything from communication and education to entertainment and healthcare. Despite its many benefits, the Internet also presents challenges such as cybersecurity threats and privacy concerns. Addressing these challenges requires ongoing efforts to enhance security, protect user data, and ensure equitable access to digital technologies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that decreasing the length penalty to 0.5 rlly helped, else most of the outputs would end abruptly without the completion of the sentence"
      ],
      "metadata": {
        "id": "DUHHBaw6Qrkt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abPtyVKsQyF2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}